{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qncY3FktdgMI"
      },
      "source": [
        "# DL4ASP - Lab Assignment 3 - Automatic Speech Recognition (2023-24)\n",
        "\n",
        "By Doroteo Torre Toledano & Beltrán Labrador Serrano.\n",
        "\n",
        "**Based on IS09 TUTORIAL \"Advanced methods for neural end-to-end speech processing – unification, integration, and <span style=\"color:red\">implementation</span> -\"**\n",
        "By: [Shigeki Karita](https://github.com/ShigekiKarita);\n",
        "NTT Communication Science Laboratories;\n",
        "15, September, 2019\n",
        "\n",
        "**Based on CMU 11751/18781 Fall 2022: ESPnet Tutorial\"**\n",
        "By: [Yifan Peng](yifanpen@andrew.cmu.edu); https://espnet.github.io/espnet/notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6yNO58J-7my"
      },
      "source": [
        "# Abstract\n",
        "\n",
        "This lab assignment introduces the task of Automatic Speech Recognition, ASR (also known as Speech-To-Text, STT), which aims to assign an output sequence of words, letters of phonemes to an input sequence of audio (or audio features).\n",
        "\n",
        "Speech Recognition has evolved from template-matching methods (such as Dynamic Time Warping, DTW) in the early beginnings to statistical-based methods (such as Hidden Markov Models (HMMs) combined with Gaussian Mixture Models (GMMs)). In the last years, it is dominated by deep learning methods, first combined with the previous HMM machinery (Hybrid HMM-DNN methods), and more recently solving the whole sequence-to-sequence problem with end-to-end deep learning approaches.\n",
        "\n",
        "In this lab assignment you will learn to build one of the most modern end-to-end deep learning approaches with a recent tool developed exclusively for sequence-to-sequence problems. [ESPnet](https://github.com/espnet/espnet) is a widely-used end-to-end speech processing toolkit that currently supports various speech processing tasks. ESPnet uses PyTorch as its main deep learning engine, and also follows Kaldi style recipes to provide a complete setup for speech recognition and other speech processing experiments.\n",
        "\n",
        "## Sessions & Report\n",
        "The lab assignment will consist of two sessions, during which a report will be elaborated by each student answering to the questions proposed in this notebook under the titles **'Questions for the report'**. The report will be delivered after the last session and before the last day of the course.\n",
        "To answer the questions proposed for the report you may need to:\n",
        "-\tSearch the ESPnet online documentation (https://espnet.github.io/espnet).\n",
        "-\tSee the output of the scripts (both in the Google Colab notebook and in the files/folders generated by each stage).\n",
        "\n",
        "**NOTE:** Running some parts of the code takes a lot of time. In particular, training the neural network and decoding can take over 20 minutes and scoring can take over 30. Therefore, it is advisable to run all the code, and continue answering the questions while the code is running.\n",
        "\n",
        "\n",
        "## Materials:\n",
        "- [ESPnet repository](https://github.com/espnet/espnet)\n",
        "- [ESPnet documentation](https://espnet.github.io/espnet/)\n",
        "- These slides https://github.com/espnet/interspeech2019-tutorial\n",
        "- API documetation https://espnet.github.io/espnet/\n",
        "- TIMIT corpus (**can only be used for this lab assignment**) (https://drive.google.com/open?id=14Nz-80FDX4G6oY6UeluKc0zDxdP5Y73-)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl9JFMNJ5iYu"
      },
      "source": [
        "## Objectives\n",
        "After this tutorial, you are expected to know how to:\n",
        "- Run existing recipes (data prep, training, inference and scoring) in ESPnet2\n",
        "- Change the training and decoding configurations\n",
        "\n",
        "Optional extensions:\n",
        "- Modifying the training and decoding configurations (e.g. other features or models)\n",
        "- Using pre-trained models\n",
        "- Recognizing your own voice\n",
        "- Using real-time decoding\n",
        "- Other ideas...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGg1N9jufpf2"
      },
      "source": [
        "## Useful links\n",
        "\n",
        "- Installation https://espnet.github.io/espnet/installation.html\n",
        "- Usage https://espnet.github.io/espnet/espnet2_tutorial.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5KGSE-w-6Hk"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. ESPnet Overview\n",
        "1. ESPnet Installation\n",
        "1. Download and exploration of TIMIT dataset\n",
        "1. Runing an existing recipe step by step\n",
        "1. Optional Extensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAOHL0Rq-6Hm"
      },
      "source": [
        "# 1. ESPnet Overview\n",
        "\n",
        "ESPnet provides **bash recipes and python library** for speech processing. This lab assignment explores the bash recipes in ASR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USfjVUT8-6Hy"
      },
      "source": [
        "## 1.1 Bash recipe overview\n",
        "\n",
        "ESPnet supports **many** ASR tasks including\n",
        "\n",
        "- Multilingual ASR: en, zh, ja, etc\n",
        "- Noise robust and far-field ASR\n",
        "- Multi-channel ASR: joint training with speech enhancement\n",
        "- Speech Translation: ASR + MT\n",
        "\n",
        "It also supports a number of tasks different to ASR:\n",
        "- Text-to-speech (TTS), also known as Speech Synthesis\n",
        "- Speech Enhancement\n",
        "- Speaker recognition\n",
        "\n",
        "For more up-to-date details:\n",
        "https://github.com/espnet/espnet/tree/master/egs2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQ6iKES-6H0"
      },
      "source": [
        "## 1.2 ASR Performance\n",
        "\n",
        "Performance on different corpora is reported in the README.md files in each recipe.\n",
        "\n",
        "Some examples:\n",
        "\n",
        "https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1/README.md\n",
        "\n",
        "https://github.com/espnet/espnet/tree/master/egs2/timit/asr1/README.md\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWXnxpNbo9CI"
      },
      "source": [
        "## 1.3 Pretrained models\n",
        "\n",
        "A lot of pretrained models are available in Huggingface (filter with espnet)\n",
        "\n",
        "https://huggingface.co/models?sort=downloads&search=espnet\n",
        "\n",
        "https://huggingface.co/docs/hub/main/en/espnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C-hieaYH8v5"
      },
      "source": [
        "# 2. ESPnet Installation\n",
        "Based on https://espnet.github.io/espnet/notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html\n",
        "\n",
        "Modified by DTT (Doroteo Torre Toledano)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xJr22WooH6rj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            " Current date and time: 12/13/2023 05:21:45\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def print_date_and_time():\n",
        "  from datetime import datetime\n",
        "  import pytz\n",
        "\n",
        "  now = datetime.now(pytz.timezone(\"America/New_York\"))\n",
        "  print(\"=\" * 60)\n",
        "  print(f' Current date and time: {now.strftime(\"%m/%d/%Y %H:%M:%S\")}')\n",
        "  print(\"=\" * 60)\n",
        "\n",
        "# example output\n",
        "print_date_and_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khr4h09Mc1Ei"
      },
      "source": [
        "**NOTE: You will need a GPU to run this code. Please check that you have selected the appropriate running environment in Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mx_ClzA3IbRt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Dec 13 11:21:54 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.223.02   Driver Version: 470.223.02   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
            "|  0%   43C    P8    12W / 170W |   2789MiB / 12050MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1015      G   /usr/lib/xorg/Xorg                247MiB |\n",
            "|    0   N/A  N/A      1184      G   /usr/bin/gnome-shell                9MiB |\n",
            "|    0   N/A  N/A      2056      C   ...vs/pytorch-env/bin/python     2529MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WMqC168VIdNd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'espnet' already exists and is not an empty directory.\n",
            "/home/javiermunoz/Universidad/MasterDeepLearning/DL4ASP/Practica/Lab3/espnet\n"
          ]
        }
      ],
      "source": [
        "# It takes a few seconds\n",
        "#!git clone --depth 5 https://github.com/espnet/espnet\n",
        "\n",
        "# We use a specific commit just for reproducibility.\n",
        "# (DTT) - This git checkout does not work, so we comment it and use the last commit\n",
        "#%cd ./espnet\n",
        "#!git checkout 3a22d1584317ae59974aad62feab8719c003ae05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5_4TE38bI2et"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: './espnet/tools'\n",
            "/home/javiermunoz/Universidad/MasterDeepLearning/DL4ASP/Practica/Lab3/espnet\n",
            "/bin/bash: ./setup_anaconda.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# It takes 35 seconds\n",
        "#%cd ./espnet/tools\n",
        "#!./setup_anaconda.sh anaconda espnet 3.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aAp-NJ8I6ld"
      },
      "outputs": [],
      "source": [
        "# It may take 12 minutes\n",
        "#%cd ./espnet/tools\n",
        "#!make TH_VERSION=1.12.1 CUDA_VERSION=11.6 # 11.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31xvIrC6M7Cs"
      },
      "source": [
        "After the main installation of ESPnet, we can install optional packages. However, no additional packages are required for this lab assignment.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--3pfvIpNSl4"
      },
      "source": [
        "If other listed packages are necessary, you can install any of them using\n",
        "\n",
        "`. ./activation_python.sh && ./installers/install_xxx.sh`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "662COFr-Ncln"
      },
      "outputs": [],
      "source": [
        "# s3prl and fairseq are necessary if you want to use self-supervised pre-trained models\n",
        "# It takes 50s\n",
        "# We do not install them by default\n",
        "#%cd /content/espnet/tools\n",
        "\n",
        "#!. ./activate_python.sh && ./installers/install_s3prl.sh      # install s3prl to use SSLRs\n",
        "#!. ./activate_python.sh && ./installers/install_fairseq.sh    # install s3prl to use Wav2Vec2 / HuBERT model series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhCXWW_2Nclo"
      },
      "source": [
        "Now let’s make sure torch, torch cuda, and espnet are successfully installed. We should get someting like\n",
        "\n",
        "`...`\n",
        "\n",
        "`[x] torch=1.12.1`\n",
        "\n",
        "`[x] torch cuda=11.6`\n",
        "\n",
        "`[x] torch cudnn=8302`\n",
        "\n",
        "`...`\n",
        "\n",
        "`[x] espnet=202310`\n",
        "\n",
        "`...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8jTW7AcNclp"
      },
      "outputs": [],
      "source": [
        "%cd ./espnet/tools\n",
        "!. ./activate_python.sh && python3 check_install.py | head -n 40\n",
        "\n",
        "# NOTE: Checkpoint 1\n",
        "print_date_and_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JO1VXAJvaF"
      },
      "source": [
        "#3. Download and explore the TIMIT dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdRNpYHip3aP"
      },
      "source": [
        "\n",
        "We will use the TIMIT speech corpus for training and testing the ASR system.\n",
        "\n",
        "**WARNING: You can use TIMIT only for the purpose of this lab assignment. You are not allowed not copy, distribute or use this corpus for any other task.**\n",
        "\n",
        "In this section we will download the corpus and explore the different types of file it contains.\n",
        "\n",
        "Download the TIMIT corpus with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CClBK1d7TQJ"
      },
      "outputs": [],
      "source": [
        "# It takes 25 seconds\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "# Alternative download location, in case first is not working\n",
        "#!gdown -O TIMIT.tgz https://drive.google.com/uc?id=1JAbQTuumm_0CIXciBkeyWtQeJ-7yFh5Y\n",
        "# %cd /content\n",
        "!gdown -O TIMIT.tgz https://drive.google.com/uc?id=1cpnMCdFdQuWINW-eEjiGIYBLOSr_5Nz9\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDs3dl31O-De"
      },
      "outputs": [],
      "source": [
        "# It takes 12 seconds\n",
        "#%cd /content\n",
        "\n",
        "!tar -xzf TIMIT.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_IXrQ2m7WXn"
      },
      "source": [
        "\n",
        "Now have a look at the contents of the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAcUQtSOu_tW"
      },
      "outputs": [],
      "source": [
        "!apt-get install tree\n",
        "!tree -L 1 /content/timit/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIaoaSmg99Kt"
      },
      "source": [
        "This directory contains 3 subdirectories, of which only the TIMIT subdirectory is of interest for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYPedFXQ-NCL"
      },
      "outputs": [],
      "source": [
        "!tree -L 2 /content/timit/TIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuBcZ1Jh-d8W"
      },
      "source": [
        "In this directory, you will find one directory with documentation (DOC) and two containing the real data, divided into training (TRAIN) and test (TEST). These two last directories share the same structure, but we need to introduce a few concepts about the dataset before explaining this structure.\n",
        "\n",
        "TIMIT contains a total of 6300 sentences, 10 sentences spoken by each of 630 speakers from 8 major dialect regions of the United States.\n",
        "```\n",
        "      Dialect\n",
        "      Region(dr)    #Male    #Female    Total\n",
        "      ----------  --------- ---------  ----------\n",
        "         1         31 (63%)  18 (27%)   49 (8%)  \n",
        "         2         71 (70%)  31 (30%)  102 (16%)\n",
        "         3         79 (67%)  23 (23%)  102 (16%)\n",
        "         4         69 (69%)  31 (31%)  100 (16%)\n",
        "         5         62 (63%)  36 (37%)   98 (16%)\n",
        "         6         30 (65%)  16 (35%)   46 (7%)\n",
        "         7         74 (74%)  26 (26%)  100 (16%)\n",
        "         8         22 (67%)  11 (33%)   33 (5%)\n",
        "       ------     --------- ---------  ----------\n",
        "        ALL       438 (70%) 192 (30%)  630 (100%)\n",
        "\n",
        "```\n",
        "\n",
        "There are three different types of sentences according to the type of text and the number of speakers that read them:\n",
        "```\n",
        "  Sentence Type   #Sentences   #Speakers   Total   #Sentences/Speaker\n",
        "  -------------   ----------   ---------   -----   ------------------\n",
        "  Dialect (SA)          2         630       1260           2\n",
        "  Compact (SX)        450           7       3150           5\n",
        "  Diverse (SI)       1890           1       1890           3\n",
        "  -------------   ----------   ---------   -----    ----------------\n",
        "  Total              2342                   6300          10\n",
        "```\n",
        "The corpus is divided into a training part and a test part in a way that:\n",
        "-\tA speaker appears in either train or test, but not in both.\n",
        "-\tAll sentences appearing in test are different from those in train.\n",
        "  -\tDialect sentences (SA) are excluded from test (because they are read by both all train and all test speakers).\n",
        "\n",
        "\n",
        "For each dialectal region (DRN) we have a number of speakers according to the following structure:\n",
        "```\n",
        "<DIALECT>/<SEX><SPEAKER_ID>\n",
        "```\n",
        "Where:\n",
        "```\n",
        "DIALECT :== DR1 | DR2 | DR3 | DR4 | DR5 | DR6 | DR7 | DR8\n",
        "SEX :== M | F\n",
        "SPEAKER_ID :== <INITIALS><DIGIT>\n",
        "    INITIALS :== speaker initials, 3 letters\n",
        "    DIGIT :== number 0-9 to differentiate speakers with\n",
        "              identical initials                              \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlMXME_p-mvC"
      },
      "outputs": [],
      "source": [
        "!tree -L 1 /content/timit/TIMIT/TRAIN/DR1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuTRNlKi-zhr"
      },
      "source": [
        "For each speaker we have several utterances, and several files for each utterance according to the following file naming convention:  \n",
        "```\n",
        "<SENTENCE_ID>.<FILE_TYPE>\n",
        "```\n",
        "Where:\n",
        "```\n",
        "SENTENCE_ID :== <TEXT_TYPE><SENTENCE_NUMBER>             \n",
        "    TEXT_TYPE :== SA | SI | SX\n",
        "    SENTENCE_NUMBER :== 1 ... 2342                \n",
        "FILE_TYPE :== WAV | TXT | WRD | PHN\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgWK10Bb-5nw"
      },
      "outputs": [],
      "source": [
        "!tree -L 1 /content/timit/TIMIT/TRAIN/DR1/FCJF0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CECi2s2G_EjD"
      },
      "source": [
        "For each utterance we have:\n",
        "- The transcripton (.TXT): orthographic transcription of the utterance (with start and end audio sample of the utterance).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnaHixjoMW0E"
      },
      "outputs": [],
      "source": [
        "!cat /content/timit/TIMIT/TRAIN/DR1/FCJF0/SX397.TXT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhQhxZwwMM2B"
      },
      "source": [
        "- The time-aligned word transcripton (.WRD) (with start and end audio sample of each word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4A-Ua9uL0dM"
      },
      "outputs": [],
      "source": [
        "!cat /content/timit/TIMIT/TRAIN/DR1/FCJF0/SX397.WRD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4IPHhQqMJEM"
      },
      "source": [
        "- The time-aligned phone transcription (.PHN) (with start and end audio sample of each phone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glaAGHowMFhT"
      },
      "outputs": [],
      "source": [
        "!cat /content/timit/TIMIT/TRAIN/DR1/FCJF0/SX397.PHN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sejs21G_LwW6"
      },
      "source": [
        "- The speech audio recording in SPHERE wav format (.WAV).\n",
        "   The SPHERE format is not a standard audio format.\n",
        "   You need to convert it to a standard wav format to listen to it, for instance with SOX.\n",
        "\n",
        "The TIMIT corpus is a very special corpus. Normally you don’t have time-alignments at all and you don’t have phonetic transcriptions: in standard corpus you only have the .wav and the .txt files, or even the .wav files only!\n",
        "\n",
        "Take a look at the audio file characteristics and listen to a few of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49gAEqFQMxXC"
      },
      "outputs": [],
      "source": [
        "# It takes 12 seconds\n",
        "!apt-get install sox\n",
        "!mkdir -p /content/tmpdata\n",
        "!sox /content/timit/TIMIT/TRAIN/DR1/FCJF0/SX397.WAV /content/tmpdata/SX397.wav\n",
        "import wave\n",
        "obj = wave.open('/content/tmpdata/SX397.wav','r')\n",
        "print( \"Number of channels\",obj.getnchannels())\n",
        "print ( \"Sample width\",obj.getsampwidth())\n",
        "print ( \"Frame rate.\",obj.getframerate())\n",
        "print (\"Number of frames\",obj.getnframes())\n",
        "obj.close()\n",
        "from scipy.io import wavfile\n",
        "import matplotlib.pyplot as pyplot\n",
        "import IPython\n",
        "samplerate, data = wavfile.read('/content/tmpdata/SX397.wav')\n",
        "\n",
        "#Plot file\n",
        "pyplot.plot(data)\n",
        "pyplot.show()\n",
        "\n",
        "# Play wave (May not workd depending on browser, but you can download and play the file)\n",
        "IPython.display.Audio('/content/tmpdata/SX397.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lh6xd2rcSA1"
      },
      "source": [
        "Now let's have a look at the Mel-frequency spectrogram of this file. This is very similar to the features that the speech recognizer takes as input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OClOnOUuDWNj"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "y, sr = librosa.load('/content/tmpdata/SX397.wav')\n",
        "melspec=librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "pyplot.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(librosa.power_to_db(melspec), y_axis='mel', fmax=8000, x_axis='time')\n",
        "pyplot.colorbar(format='%+2.0f dB')\n",
        "pyplot.title('Mel spectrogram')\n",
        "pyplot.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTOX5seYzaBe"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q3.1):*\n",
        "\n",
        "-\tExplore the TRAIN and TEST directory and try to listen to some files checking that the audio is the expected one (gender, text, dialectal region if you can recognize the dialect, etc.). You can do this by modifying the corresponding parts of the Google Colab notebook. Provide one example in the report including gender, text and dialectal region.\n",
        "-\tCheck the contents of the .txt, .wrd and .phn files against the audio (.wav) file for a few files. You can do this by modifying the corresponding parts of the Google Colab notebook. Include in the report (a few lines of ) these files for the example in the previous question.\n",
        "-\tExplore the documentation of the TIMIT corpus and try to find other information associated with the speakers. Can you imagine other applications of this corpus beyond training and testing Speech-To-Text systems? Include your answers in the report.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ArA6GfJ5et"
      },
      "source": [
        "#4. Run an existing recipe step by step\n",
        "\n",
        "\n",
        "## 4.1 KALDI-style recipes and directory structure\n",
        "ESPnet follows the KALDI-style recipe structure in Bash for ASR systems.\n",
        "This structure has changed somehow with the new version (ESPnet2) but is still inspired in the KALDI-style recipe structure.\n",
        "\n",
        "There is a directory for each corpus under the directory `espnet/egs2/`.\n",
        "\n",
        "Have a look at the different recipes contained in ESPnet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yXGizSIgOkg"
      },
      "outputs": [],
      "source": [
        "!ls /content/espnet/egs2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gdLjrQQgnb6"
      },
      "source": [
        "In this lab assingment we will be working with the recipe for the TIMIT database.\n",
        "\n",
        "It is located in `espnet/egs2/timit/asr1`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V_-mPe0goyG"
      },
      "outputs": [],
      "source": [
        "!tree -L 1 /content/espnet/egs2/timit/asr1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHuvVNGWn5mP"
      },
      "source": [
        "All the recipes have the same structure:\n",
        "```\n",
        " - conf/      #Configuration files for training, inference, etc.\n",
        " - scripts/   # Bash utilities of espnet2\n",
        " - pyscripts/ # Python utilities of espnet2\n",
        " - steps/     # From Kaldi utilities\n",
        " - utils/     # From Kaldi utilities\n",
        " - db.sh      # The directory path of each corpora\n",
        " - path.sh    # Setup script for environment variables\n",
        " - cmd.sh     # Configuration for your backend of job scheduler\n",
        " - run.sh     # Entry point\n",
        " - asr.sh     # Invoked by run.sh\n",
        "```\n",
        "\n",
        "You need to modify `db.sh` for specifying your corpus before executing `run.sh`. For example, when you touch the recipe of `egs2/timit`, you need to change the paths of TIMIT in `db.sh` (but don't worry about this because this notebook does this for you later!)\n",
        "\n",
        "Some corpora can be freely obtained from the WEB and they are written as `downloads/` at the initial state. You can also change them to your corpus path if it’s already downloaded.\n",
        "\n",
        "`path.sh` is used to set up the environment for `run.sh`. Note that the Python interpreter used for ESPnet is not the current Python of your terminal, but it’s the Python which was installed at `tools/`. Thus you need to source `path.sh` to use this Python.\n",
        "\n",
        "```\n",
        " . path.sh\n",
        "    python\n",
        "```\n",
        "`cmd.sh` is used for specifying the backend of the job scheduler. If you don’t have such a system in your local machine environment, you don’t need to change anything about this file. You definitely do not have to change this file in Google colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW4vo_ZCpqKl"
      },
      "source": [
        "`conf` is a directory containing configuration files for the recipe. We will start with the standard configuration, but we have to modify it to make different experiments. Have a look at its content and try to figure out the use of each file (their names help)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE_jRa59qUAE"
      },
      "outputs": [],
      "source": [
        "!tree -L 1 /content/espnet/egs2/timit/asr1/conf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyyLRKaXq2Xy"
      },
      "source": [
        "\n",
        "> One of the most important configuation files is the one defining the neural network architecture and the training parameters, which are defined in `train_asr.yaml`. Examine the file and try to figure out what are the different parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrJSy7iDrRbw"
      },
      "outputs": [],
      "source": [
        "!cat /content/espnet/egs2/timit/asr1/conf/train_asr.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTACBqZbriko"
      },
      "source": [
        "> Similarly, `decode.yaml` contains parameters for decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YGqTykXrY6r"
      },
      "outputs": [],
      "source": [
        "!cat /content/espnet/egs/timit/asr1/conf/decode.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxStyIkIpTZ-"
      },
      "source": [
        "- `README.md` contains the results obtained with this recipe with different configurations. Explore the different results obtained with this database. You have to look at the `Sum/Avg` row. The columns represent (from left-to-right):\n",
        ">- Number of files.\n",
        ">- Number of words.\n",
        ">- % Correct tokens (phones/characters/words depending on the token units used in the configuration).\n",
        ">- % Substituted tokens.\n",
        ">- % Deleted tokens.\n",
        ">- % Inserted tokens.\n",
        ">- Phone / Character / Word Error Rate in %.\n",
        ">- Sentence Error Rate in %.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izvak_o5nCy4"
      },
      "outputs": [],
      "source": [
        "!cat /content/espnet/egs2/timit/asr1/README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKrMO2-w34q"
      },
      "source": [
        "`run.sh` is the main script, which we often call as “recipe”, to run all stages related to DNN experiments; data-preparation, training, and evaluation.\n",
        "\n",
        "You can execute all the recipe by just running\n",
        "```\n",
        "%cd /content/espnet/egs2/timit/asr1\n",
        "!./run.sh\n",
        "```\n",
        "\n",
        "However, we will be executing the recipe stage by stage to show and explain the different parts of the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZM6MA23RRb7"
      },
      "source": [
        "First of all, we need to tell ESPnet where is the TIMIT corpus.\n",
        "We do this by modifying the `db.sh` file in the recipe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4l9vYqlRQr7"
      },
      "outputs": [],
      "source": [
        "%cd /content/espnet/egs2/timit/asr1\n",
        "!cat db.sh | grep TIMIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-M9gGE2aqtC"
      },
      "outputs": [],
      "source": [
        "!mv db.sh db.sh.bk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxeM0FHfR0t2"
      },
      "outputs": [],
      "source": [
        "!sed -e \"s/TIMIT=/TIMIT=\\/content\\/timit\\/TIMIT/\" db.sh.bk > db.sh\n",
        "!cat db.sh | grep TIMIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-jwEZuAz2bV"
      },
      "source": [
        "First, let's have a look at the main script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk0P7WINTb1m"
      },
      "outputs": [],
      "source": [
        "!cat run.sh\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8TTZNSQz79h"
      },
      "source": [
        "The main script just calls the `asr.sh` script with some configurations.\n",
        "So, let's have a look at the `asr.sh` script as well (it is very long and it is not necessary to understand it all)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOZ008c60JBK"
      },
      "outputs": [],
      "source": [
        "!cat asr.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSo90_-C0pTz"
      },
      "source": [
        "`asr.sh` is quite long, but it is structured in stages and you don't need to know exactly how all of them work. It is important, however, to understand the different stages.\n",
        "\n",
        "You can run the stages you want by providing the arguments `--stage` and `--stop_stage` to both, the `run.sh` and/or the `asr.sh` scripts.\n",
        "\n",
        "For instance, the following command will execute stages 2 to 5:\n",
        "\n",
        "`!./run.sh --stage=2 --stop_stage=5`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaOp4UP1uKP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1):*\n",
        "\n",
        "-\tWhat types of recipes (recipes for different purposes such as ASR, speech synthesis, etc.) can you identify in the egs2 directory?\n",
        "-\tWhat files will you need to change to change the structure of the DNNs used by the STT? (e.g. adding more layers in encoder)\n",
        "- What files will you need to change to change the Sequence-to-Sequence mapping technique used? (e.g. only CTC or only Attention-based encoder-decoder)\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YPheDuo0oxa"
      },
      "source": [
        "###4.1.1 Stage 1: Data Preparation\n",
        "\n",
        "The first stage is completely corpus dependent. Its main purpose is to process the data in a specific corpus (mainly speech in some audio format and labels in text format) and organize it in a uniform way, so that all the recipes can proceed from a unified data format.\n",
        "\n",
        "Note that `--stage <N>` is to specify the starting stage and `--stop_stage <N>` is to specifiy the stopping stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4P6Wv3eTsnk"
      },
      "outputs": [],
      "source": [
        "# It takes 30 seconds.\n",
        "!./run.sh --stage 1 --stop_stage 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ORceFPOkLQJ"
      },
      "source": [
        "After this stage is finished, please check the `data` directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQrvqE-aUiQ6"
      },
      "outputs": [],
      "source": [
        "%cd /content/espnet/egs2/timit/asr1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLY4zuPFiAWK"
      },
      "outputs": [],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S85-3X82kWbm"
      },
      "source": [
        "In this recipe, we use `train` as a training set, `dev` as a validation set (monitor the training progress by checking the validation score). We also use (reuse) `test` and `dev` sets for the final speech recognition evaluation.\n",
        "\n",
        "Let's check one of the training data directory:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyAbGjDElKFA"
      },
      "outputs": [],
      "source": [
        "!ls -1 data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mob1Pd_ylPyb"
      },
      "source": [
        "These are the speech and corresponding text and speaker information based on the Kaldi format. Please also check https://kaldi-asr.org/doc/data_prep.html\n",
        "```\n",
        "spk2utt # Speaker information\n",
        "text    # Transcription file\n",
        "utt2spk # Speaker information\n",
        "wav.scp # Audio file\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnWSK6K73sc9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1.1):*\n",
        "\n",
        "- What is the purpose of stage 1?\n",
        "- Which are the 3 subsets of TIMIT used? How many utterances has each one of them?\n",
        "- What type of transcriptions is used in this recipe?. Can you use another type? Which one?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVom1NvZ6Mnx"
      },
      "source": [
        "###4.1.2 Stage 2: Speed perturbation (one of the data augmentation methods)\n",
        "\n",
        "We do not use speed perturbation for this demo. But you can turn it on by adding an argument `--speed_perturb_factors \"0.9 1.0 1.1\"` to the shell script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj7lmkC_2s93"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 2 --stop_stage 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ2m5s5l4KG7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1.2):*\n",
        "\n",
        "- What is “speed perturbation”? Do you think that it may help in this case?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EAZ0E_s6Zdy"
      },
      "source": [
        "###4.1.3 Stage 3: Format wav.scp: data/ -> dump/raw\n",
        "\n",
        "We dump the data with specified format (flac in this case) for the efficient use of the data.\n",
        "\n",
        "Note that adding `--nj <N>` you can specify the number of CPU jobs. Please set it appropriately by considering your CPU resources and disk access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-zwuTnu20-g"
      },
      "outputs": [],
      "source": [
        "# It takes 2 minutes\n",
        "!./run.sh --stage 3 --stop_stage 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P3gdQMR7H88"
      },
      "source": [
        "###4.1.4 Stage 4: Remove long/short data: dump/raw/org -> dump/raw\n",
        "\n",
        "There are too long and too short audio data, which are harmful for our efficient training. Those data are removed from the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM_KSu6t2-wK"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 4 --stop_stage 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG-spW3z4m6h"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1.4):*\n",
        "\n",
        "- Stage 4 removes very long o very short segments for efficiency reasons. Does it do this to all the subsets? Why?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtrtBRPe7Ygz"
      },
      "source": [
        "###4.1.5 Stage 5: Generate token_list from dump/raw/train/text using BPE.\n",
        "\n",
        "Byte Pair Encoding (BPE) is a way of compressing the information in a vocabulary by finding frequent sub-words.\n",
        "\n",
        "This is important for text processing. We make a dictionary based on the English character in this example.\n",
        "\n",
        "We use a `sentencepiece` toolkit developed by Google.\n",
        "\n",
        "**NOTE:** This lab assignment does not use BPE since the task is phonetic recognition. \"Words\" are really phones in this lab assignment, and there is a finite and very limited number of different phones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxQUodCo3GQ_"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 5 --stop_stage 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k1pSh1wyUJc"
      },
      "source": [
        "Let's check the content of the dictionary. There are several special symbols, e.g.,\n",
        "\n",
        "```\n",
        "<blank>   used for CTC\n",
        "<unk>     unknown symbols do not appear in the training data\n",
        "<sos/eos> start and end sentence symbols\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBEVQ9eOdeGP"
      },
      "outputs": [],
      "source": [
        "!cat data/token_list/word/tokens.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JVEetL04_GG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1.5):*\n",
        "\n",
        "- Although not used in this example, find the meaning of g2p (a parameter of the main script of this stage) and explain it in the report.\n",
        "-\tFind an example output of this stage for one particular file of the training corpus and include it in the report.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y5IlGWz7sXd"
      },
      "source": [
        "###4.1.6-9 language modeling (skip in this tutorial)\n",
        "\n",
        "**Stages 6--9: Stages related to language modeling.**\n",
        "\n",
        "We skip the language modeling part in the recipe (stages 6 -- 9) in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vNbU_0l3gXM"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 6 --stop_stage 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITV6Zn4y3j-_"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 7 --stop_stage 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0hck9vK3nLU"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 8 --stop_stage 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YisLK7y3qUe"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 9 --stop_stage 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68olkUML5dF3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1.9):*\n",
        "\n",
        "- What is language modelling? Why is it normally used in ASR?\n",
        "- Why language modelling is not included in this recipe?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OASA_sOQ71M6"
      },
      "source": [
        "###4.1.10 End-to-end ASR ASR collect stats\n",
        "\n",
        "We estimate the mean and variance of the data to normalize the data. We also collect the information of input and output lengths for the efficient mini batch creation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g54UlkL3uM0"
      },
      "outputs": [],
      "source": [
        "# It takes 1 minute\n",
        "!./run.sh --stage 10 --stop_stage 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7Bse9Sd8Vu3"
      },
      "source": [
        "###4.1.11 Stage 11: ASR Training.\n",
        "\n",
        "This is the main training loop. It takes a lot of time, but you can monitor the progress in different ways while training the network.\n",
        "\n",
        "One way is to explore the following files that are actualized per epoch:\n",
        "- log file /content/espnet/egs2/timit/asr1/exp/asr_train_asr_raw_word/train.log\n",
        "- loss /content/espnet/egs2/timit/asr1/exp/asr_train_asr_raw_word/images/loss.png\n",
        "- accuracy /content/espnet/egs2/timit/asr1/exp/asr_train_asr_raw_word/images/acc.png\n",
        "\n",
        "There are many more files to monitor progress, but these are the most important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahij3yekF5tu"
      },
      "source": [
        "You can also use Tensorboard to analyze the training and validation logs while training the network.\n",
        "\n",
        "**NOTE:** You need to launch Tensorboard before starting training. It will not found any log, of course. You need to refresh it from time to time (by clicking the reload icon) to analyze how training is progressing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfoxZlkWfbmM"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# Launch Tensorboard before training starts\n",
        "%tensorboard --logdir /content/espnet/egs2/timit/asr1/exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktTKypYC34oE"
      },
      "outputs": [],
      "source": [
        "# It takes 15 minutes\n",
        "!./run.sh --stage 11 --stop_stage 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TBlY42U6DVN"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1.11):*\n",
        "\n",
        "- Have a look at the different options for the `train.yaml` file in ESPnet documentation. Read completely the `train.yaml` file and try to identify what parameter would you change to modify:\n",
        "  -\tThe number of layers in the encoder.\n",
        "  - The type of the encoder.\n",
        "  - The number of units in each encoder layer.\n",
        "  -\tThe type of attention in the decoder.\n",
        "  -\tThe number of units in the decoder.\n",
        "- Have a look at the different graphs **in Tensorflow** and answer the following questions:\n",
        "  - Can you identify the parameters (e.g. the loss) corresponding to CTC and Attention? Include them in the report.\n",
        "  - Check the evolution of the different parameters (particularly the loss and the accuracy) during training and compare the evolution of the results in train and validation. Do you think that training worked properly?\n",
        "- An alternative way to check the evolution of training is using the graphs generated during the training process in /content/espnet/egs2/timit/asr1/exp/asr_train_asr_raw_word/. You can have a look at them during training using the file explorer, but you need to wait until training ends to answer these questions:\n",
        "  - Can you see a difference in the evolution of the CTC and the Attention loss? Can you explain this different behavior?\n",
        "  - What is the final Phone Error Rate (PER) achieved in train and validation? What is the difference between them?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a-NSbgoPrvp"
      },
      "source": [
        "###4.1.12 Stage 12: Decoding.\n",
        "\n",
        "Note that you can use `inference_nj <N>` to specify the number of inference jobs\n",
        "\n",
        "Let's monitor the log /content/espnet/egs2/timit/asr1/exp/asr_train_asr_raw_word/decode_asr_asr_model_valid.acc.ave/dev/logdir/asr_inference.1.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oUrbB-k8xgf"
      },
      "outputs": [],
      "source": [
        "# It takes 30 minutes\n",
        "!./run.sh --stage 12 --stop_stage 12 --inference_nj 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44cm0UfTFOD"
      },
      "source": [
        "###4.1.13: Stage 13: Scoring\n",
        "\n",
        "This stage computes the word error rate (WER), character error rate (CER), etc. for each test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ae3SiyR82JH"
      },
      "outputs": [],
      "source": [
        "# It takes 13 seconds\n",
        "!./run.sh --stage 13 --stop_stage 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q1O3M9E_fmk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#*Questions for the report (Q4.1.13):*\n",
        "\n",
        "- Explore the file `decode.yaml` and answer the following questions:\n",
        "  -\tWhat is the “beam-size” parameter?\n",
        "  - What is the “ctc-weigth” parameter?\n",
        "- Explore the output of the scoring phase and answer the following questions:\n",
        "  - What is the meaning of the different values produced by the scoring software and included in the heading `|dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err|`?.\n",
        "  - Find the final Phone Error Rate (equivalent to the Word Error Rate in this experiment) for the two sets evaluated.\n",
        "- You can also check the breakdown of the phone error rate in /content/espnet/egs2/timit/asr1/exp/asr_train_asr_raw_word/decode_asr_asr_model_valid.acc.ave/dev/score_wer/result.txt. Explore this file and answer the following questions:\n",
        "  - Find the best and the worst phone error rate for the different speakers. Do you think that these results are very different? Do you think it is normal? Why?\n",
        "  -\tExplore the file until it starts dumping the alignments between hypothesis and reference. Compute manually the PER for the second alignment appearing (sentence fadg0-fadg0_si1909). What is the PER for that file?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCJ-sjlcIhuq"
      },
      "source": [
        "###4.1.14 Stage 14: Packing the model for uploading\n",
        "\n",
        "ESPnet scripts are prepared to pack the generated model and upload it to Zenodo/Huggingface. This stage packs the model.\n",
        "\n",
        "We skip this stage in this lab assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmH0ePtu83gn"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 14 --stop_stage 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdZieZpUJAaQ"
      },
      "source": [
        "###4.1.15 Stage 15: Uploading the model to Zenodo\n",
        "\n",
        "ESPnet scripts are prepared to pack the generated model and upload it to Zenodo. This stage uploads the packed model.\n",
        "\n",
        "We skip this stage in this lab assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wThhWqz784Bn"
      },
      "outputs": [],
      "source": [
        "#!./run.sh --stage 15 --stop_stage 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTDYVvSbJqPb"
      },
      "source": [
        "###4.1.15 Stage 15: Uploading the model to Huggingface\n",
        "\n",
        "ESPnet scripts are prepared to pack the generated model and upload it to Huggingface. This stage uploads the packed model.\n",
        "\n",
        "We skip this stage in this lab assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg0EBI2J9J2T"
      },
      "outputs": [],
      "source": [
        "#!./run.sh --stage 16 --stop_stage 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6KgBNYkDAg9"
      },
      "source": [
        "# 4.2 Upload results to google drive\n",
        "Given that Section 4.1 takes a lot of time, it is a good idea to upload the results of this section to Google Drive, so that you can resume the lab assignment without running all the previous steps following the procedure explained in Section 4.3.\n",
        "\n",
        "**NOTE: Google Colab will ask you for permissions to upload this file to your Google Drive and it will not save it unless you give that permission**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eFEIoLJDPj2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "\n",
        "%cd /content/espnet/egs2/timit\n",
        "!mkdir -p /content/drive/MyDrive/Backup_P3_DL4ASP/\n",
        "!tar -cvzf timit_asr1.tgz asr1\n",
        "shutil.copy(\"/content/espnet/egs2/timit/timit_asr1.tgz\",\"/content/drive/MyDrive/Backup_P3_DL4ASP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WtGs5VLPqMR"
      },
      "source": [
        "#4.3. Download results from Google Drive\n",
        "If yoo lose the connection and the exectution environment you can restore it (relatively) quickly by:\n",
        "- Running sections 1-3 (Install ESPnet and download TIMIT)\n",
        "- Execute the cells in this section\n",
        "\n",
        "After doing this, you should have exactly the same execution environment that you obtained running sections 1 thru 4.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_YX2MWuQsSR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "\n",
        "%cd /content/espnet/egs2/timit\n",
        "shutil.copy(\"/content/drive/MyDrive/Backup_P3_DL4ASP/timit_asr1.tgz\",\"/content/espnet/egs2/timit\")\n",
        "!rm -rf /content/espnet/egs2/timit/asr1\n",
        "!tar -xzf timit_asr1.tgz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iWoWZTIzDQh"
      },
      "source": [
        "#5. Optional Extensions\n",
        "There are a number of possible optional extensions for this lab assignment (but rest assured that you can get the highest grade without making any of these extensions).\n",
        "\n",
        "You can either complete one or more of these optional extensions as part of the lab assignment or for the final project.\n",
        "\n",
        "The first optional extension (changing the training and/or decoding config) do not change the task, but only the model or the features. Here are some examples of the modifications you can try to explore:\n",
        "-\tChanging the Encoding or Decoding network topology (adding or removing layers, making layers wider or narrower in terms of units, etc.).\n",
        "- Changing the sequence-to-sequence mapping function (using only CTC, using only Attention, using Transformer, etc.).\n",
        "\n",
        "Any of these two optional extensions, or any combination of them will yield a different PER on the same task, and will allow you to participate in an optional challenge in which the student with the best PER will be awarded with an extra point, the 2nd best with 0.8 points, the 3rd best with 0.7 points and any student completing a meaningful modification of the original system will be awarded with an extra 0.5 points in the lab assignment.\n",
        "\n",
        "The rest of the proposed optional extensions imply changing the task, therefore, results could not be compared with the original system proposed in the lab assignment. In any case, these optional extensions give you a closer idea of real applications of automatic speech recognition, and you can use these optional extensions in your final project or as a part of the final project.\n",
        "\n",
        "##5.1 Changing the training and/or decoding config\n",
        "\n",
        "###5.1.1 By changing config files\n",
        "All training options are changed by using a config file.\n",
        "\n",
        "Please check https://espnet.github.io/espnet/espnet2_training_option.html\n",
        "\n",
        "Let's first check config files prepared in the `timit` recipe\n",
        "\n",
        "```\n",
        "- LSTM-based E2E ASR /content/espnet/egs2/timit/asr1/conf/train_asr_rnn.yaml\n",
        "- Transformer based E2E ASR /content/espnet/egs2/timit/asr1/conf/train_asr_transformer.yaml\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGGJGBs8Go1"
      },
      "source": [
        "You can run\n",
        "\n",
        "**RNN**\n",
        "```\n",
        "./asr.sh --stage 10 \\\n",
        "   --train_set train_nodev \\    \n",
        "   --valid_set train_dev \\\n",
        "   --test_sets \"train_dev test\" \\\n",
        "   --nj 4 \\\n",
        "   --inference_nj 4 \\\n",
        "   --use_lm false \\\n",
        "   ----asr_config conf/train_asr_rnn.yaml\n",
        "```\n",
        "\n",
        "**Transformer**\n",
        "```\n",
        "./asr.sh --stage 10 \\\n",
        "   --train_set train_nodev \\    \n",
        "   --valid_set train_dev \\\n",
        "   --test_sets \"train_dev test\" \\\n",
        "   --nj 4 \\\n",
        "   --inference_nj 4 \\\n",
        "   --use_lm false \\\n",
        "   ----asr_config conf/train_asr_transformer.yaml\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcMGLh1-HYwr"
      },
      "source": [
        "You can also find various configs in `espnet/egs2/*/asr1/conf/`, including\n",
        "- Conformer `espnet/egs2/librispeech/asr1/conf/train_asr_confformer.yaml`\n",
        "- Wav2vec2.0 pre-trained model and fine-tuning `https://github.com/espnet/espnet/blob/master/egs2/librispeech/asr1/conf/tuning/train_asr_conformer7_wav2vec2_960hr_large.yaml`\n",
        "- HuBERT pre-trained model and fine-tuning `https://github.com/espnet/espnet/blob/master/egs2/librispeech/asr1/conf/tuning/train_asr_conformer7_hubert_960hr_large.yaml`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYzNLITz7wyG"
      },
      "source": [
        "###5.1.2 By using command line arguments\n",
        "\n",
        "You can also customize it by editing the file or passing the command line arguments, e.g.,\n",
        "\n",
        "```\n",
        "./run.sh --stage 10 --asr_args \"--model_conf ctc_weight=0.3\"\n",
        "```\n",
        "```\n",
        "./run.sh --stage 10 --asr_args \"--optim_conf lr=0.1\"\n",
        "```\n",
        "\n",
        "See https://espnet.github.io/espnet/espnet2_tutorial.html#change-the-configuration-for-training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CI94vSCX6sz"
      },
      "source": [
        "##5.2. Using pre-trained models, recognizing your own voice or real-time decoding\n",
        "\n",
        "See the following link for how to use pre-trained models (from espnet_model_zoo), recognizing other audio files or your own voice and using real-time decoding (which requires specifica models).\n",
        "\n",
        "https://espnet.github.io/espnet/notebook/espnet2_asr_realtime_demo.html\n",
        "\n",
        "You can also use models downloaded from Hugginface. Check this webpage for more info:\n",
        "\n",
        "https://huggingface.co/docs/hub/main/en/espnet\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qncY3FktdgMI",
        "wl9JFMNJ5iYu",
        "gGg1N9jufpf2",
        "USfjVUT8-6Hy",
        "lcQ6iKES-6H0",
        "CWXnxpNbo9CI",
        "rYzNLITz7wyG",
        "5CI94vSCX6sz"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
